@phdthesis{jaeger06thesis,
    Author = {T. Florian Jaeger},
    Year = {2006},
    Title = {Redundancy and Syntactic Reduction in Spontaneous Speech},
    School = {Stanford University},
}

@article{Kuperberg2016,
  Author = {Gina N Kuperberg and T. Florian Jaeger},
  Title = {What do we mean by prediction in sentence processing?},
  Journal = {Language, Cognition, and Neuroscience},
  Volume = {31},
  Number = {1},
  Pages = {32--59},
  Year = {2016},
  doi = {10.1080/23273798.2015.1102299},
  abstract = {We consider several key aspects of prediction in language comprehension: its computational nature, the representational level(s) at which we predict, whether we use higher-level representations to predictively pre-activate lower level representations, and whether we “commit” in any way to our predictions, beyond pre-activation. We argue that the bulk of behavioural and neural evidence suggests that we predict probabilistically and at multiple levels and grains of representation. We also argue that we can, in principle, use higher-level inferences to predictively pre-activate information at multiple lower representational levels. We suggest that the degree and level of predictive pre-activation might be a function of its expected utility, which, in turn, may depend on comprehenders’ goals and their estimates of the relative reliability of their prior knowledge and the bottom-up input. Finally, we argue that all these properties of language understanding can be naturally explained and productively explored within a multi-representational hierarchical actively generative architecture whose goal is to infer the message intended by the producer, and in which predictions play a crucial role in explaining the bottom-up input.},
  keywords = {Language comprehension, prediction error, generative model, probabilistic, surprisal},
}

@article{jaeger10complementizer,
    Author = {T. Florian Jaeger},
    Year = {2010},
    Month = {August},
    Title = {Redundancy and Reduction: Speakers Manage Information Density},
    Journal = {Cognitive Psychology},
    Volume = {61},
    Number = {1},
    Pages = {23--62},
    Doi = {10.1016/j.cogpsych.2010.02.002},
    Keywords = {Efficient language production; Rational cognition; Syntactic production; Syntactic reduction; Complementizer that-mentioning},
    Abstract = {A principle of efficient language production based on information theoretic considerations is proposed: Uniform Information Density predicts that language production is affected by a preference to distribute information uniformly across the linguistic signal. This prediction is tested against data from syntactic reduction. A single multilevel logit model analysis of naturally distributed data from a corpus of spontaneous speech is used to assess the effect of information density on complementizer that-mentioning, while simultaneously evaluating the predictions of several influential alternative accounts: availability, ambiguity avoidance, and dependency processing accounts. Information density emerges as an important predictor of speakers’ preferences during production. As information is defined in terms of probabilities, it follows that production is probability-sensitive, in that speakers’ preferences are affected by the contextual probability of syntactic structures. The merits of a corpus-based approach to the study of language production are discussed as well.}
}

@inproceedings{levyjaeger07,
    Author = {Roger Levy and T. Florian Jaeger},
    Year = {2007},
    Title = {Speakers optimize information density through syntactic reduction},
    Booktitle = {Advances in Neural Information Processing Systems 19},
    Editor = {B. Schölkopf and J. Platt and T. Hoffman},
    Publisher = {MIT Press},
    Pages = {849--856},
    Address = {Cambridge, MA},
    Url = {http://books.nips.cc/papers/files/nips19/NIPS2006_0515.pdf},
}

@article{Liberman1967,
    Author = {Liberman, A. M. and Cooper, F. S. and Shankweiler, D. P. and Studdert-Kennedy, M. },
    Year = {1967},
    Month = {November},
    Title = {Perception of the speech code},
    Journal = {Psychological Review},
    Volume = {74},
    Number = {6},
    Pages = {431--461},
    Doi = {10.1037/h0020279}
}

@article{Kleinschmidt2015,
    Author = {Dave F Kleinschmidt and T. Florian Jaeger},
    Title = {Robust speech perception: Recognizing the familiar, generalizing to the similar, and adapting to the novel},
    Journal = {Psychological Review},
    Volume = {122},
    Number = {2},
    Pages = {148--203},
    DOI = {10.1037/a0038695},
    Month = {April},
    Year = {2015},
    Abstract = {Successful speech perception requires that listeners map the acoustic signal to linguistic categories. These mappings are not only probabilistic, but change depending on the situation. For example, one talker’s /p/ might be physically indistinguishable from another talker’s /b/ (cf. lack of invariance). We characterize the computational problem posed by such a subjectively nonstationary world and propose that the speech perception system overcomes this challenge by (a) recognizing previously encountered situations, (b) generalizing to other situations based on previous similar experience, and (c) adapting to novel situations. We formalize this proposal in the ideal adapter framework: (a) to (c) can be understood as inference under uncertainty about the appropriate generative model for the current talker, thereby facilitating robust speech perception despite the lack of invariance. We focus on 2 critical aspects of the ideal adapter. First, in situations that clearly deviate from previous experience, listeners need to adapt. We develop a distributional (belief-updating) learning model of incremental adaptation. The model provides a good fit against known and novel phonetic adaptation data, including perceptual recalibration and selective adaptation. Second, robust speech recognition requires that listeners learn to represent the structured component of cross-situation variability in the speech signal. We discuss how these 2 aspects of the ideal adapter provide a unifying explanation for adaptation, talker-specificity, and generalization across talkers and groups of talkers (e.g., accents and dialects). The ideal adapter provides a guiding framework for future investigations into speech perception and adaptation, and more broadly language comprehension.}
}

@inproceedings{Kleinschmidt2011,
    Author = {Dave Kleinschmidt and T. Florian Jaeger},
    Title = {A Bayesian belief updating model of phonetic recalibration and selective adaptation},
    Month = {June},
    Year = {2011},
    Booktitle = {ACL Workshop on Cognitive Modeling and Computational Linguistics},
    Address = {Portland, OR},
    Abstract = {The mapping from phonetic categories to acoustic cue values is highly flexible, and adapts rapidly in response to exposure. There is currently, however, no theoretical framework which captures the range of this adaptation. We develop a novel approach to modeling phonetic adaptation via a belief-updating model, and demonstrate that this model naturally unifies two adaptation phenomena traditionally considered to be distinct.},
    Url = {http://www.hlp.rochester.edu/publications/KleinschmidtJaeger11_cmcl.pdf}
}

@inproceedings{Kleinschmidt2012a,
    Author = {Dave F Kleinschmidt and T. Florian Jaeger},
    Month = {August},
    Year = {2012},
    Title = {A continuum of phonetic adaptation: Evaluating an incremental belief-updating model of recalibration and selective adaptation},
    Booktitle = {Proceedings of the 34th Annual Conference of the Cognitive Science Society},
    Editor = {N. Miyake and D. Peebles and R. P. Cooper},
    Keywords = {adaptation,bayesian model-,cue combination,ing,rational analysis,sentence processing},
    Publisher = {Cognitive Science Society},
    Address = {Sapporo, Japan},
    Howpublished = {talk},
    Url = {http://www.hlp.rochester.edu/publications/KleinschmidtJaeger_CogSci_2012.pdf}
}

@inproceedings{Kleinschmidt2015a,
    Author = {Kleinschmidt, Dave F and Raizada, Rajeev and Jaeger, T Florian},
    Title = {Supervised and unsupervised learning in phonetic adaptation},
    Year = {2015},
    Booktitle = {Proceedings of the 37th Annual Conference of the Cognitive Science Society},
    Pages = {1129--1134},
    Publisher = {Cognitive Science Society},
    Address = {Austin, TX},
    ISBN = {978-0-9911967-2-2},
    Editor = {Dale, Rick and Jennings, Carolyn and Maglio, Paul and Matlock, Teenie and Noelle, David and Warlaumont, Anne and Yoshimi, Jeff}
}

@article{Pajak2016,
  author = {Pajak, Bozena and Fine, Alex B and Kleinschmidt, Dave F and Jaeger, T Florian},
  journal = {Language Learning},
  title = {Learning additional languages as hierarchical inference: Insights from L1 processing},
  year = {2016},
  pages = {900--944},
  volume = {66},
  number = {4},
  issn = {1467-9922},
  doi = {10.1111/lang.12168},
  keywords = {second language acquisition, hierarchical probabilistic inference, statistical learning, speech adaptation},
  abstract = {We present a framework of second and additional language (L2/Ln) acquisition motivated by recent work on socio-indexical knowledge in first language (L1) processing. The distribution of linguistic categories covaries with socio-indexical variables (e.g., talker identity, gender, dialects). We summarize evidence that implicit probabilistic knowledge of this covariance is critical to L1 processing, and propose that L2/Ln learning uses the same type of socio-indexical information to probabilistically infer latent hierarchical structure over previously learned and new languages. This structure guides the acquisition of new languages based on their inferred place within that hierarchy and is itself continuously revised based on new input from any language. This proposal unifies L1 processing and L2/Ln acquisition as probabilistic inference under uncertainty over socio-indexical structure. It also offers a new perspective on crosslinguistic influences during L2/Ln learning, accommodating gradient and continued transfer (both negative and positive) from previously learned to novel languages, and vice versa.}
}

@inproceedings{Yildirim2013,
  author = {Ilker Yildirim and Judith Degen and Michael K. Tanenhaus and T. Florian Jaeger},
  month = {August},
  year = {2013},
  title = {Linguistic Variability and Adaptation in Quantifier Meanings},
  booktitle = {The 35th Annual Meeting of the Cognitive Science Society (CogSci13)},
  address = {Berlin, Germany},
  Url = {http://www.hlp.rochester.edu/publications/YildirimDegenTanenhausJaeger_CogSci2013.pdf}
}

@article{Yildirim2016,
  Author = {Ilker Yildirim and Judith Degen and Michael K. Tanenhaus and T. Florian Jaeger},
  Title = {Talker-specificity and adaptation in quantifier interpretation},
  Journal = {Journal of Memory and Language},
  Year = {2016},
  Volume = {87},
  Pages = {128--143},
  doi = {10.1016/j.jml.2015.08.003},
  Abstract = {Linguistic meaning has long been recognized to be highly context-dependent. Quantifiers like many and some provide a particularly clear example of context-dependence. For example, the interpretation of quantifiers requires listeners to determine the relevant domain and scale. We focus on another type of context-dependence that quantifiers share with other lexical items: talker variability. Different talkers might use quantifiers with different interpretations in mind. We used a web-based crowdsourcing paradigm to study participants’ expectations about the use of many and some based on recent exposure. We first established that the mapping of some and many onto quantities (candies in a bowl) is variable both within and between participants. We then examined whether and how listeners’ expectations about quantifier use adapts with exposure to talkers who use quantifiers in different ways. The results demonstrate that listeners can adapt to talker-specific biases in both how often and with what intended meaning many and some are used.},
  Keywords = {Adaptation; Talker-specificity; Quantifiers; Semantics; Pragmatics}
}

@inproceedings{Farmer2014,
  address = {Austin, TX : Cognitive Science Society},
  author = {Thomas A. Farmer and Alex B. Fine and Shaorang Yan and Spyridoula Cheimariou and T. Florian Jaeger},
  booktitle = {Proceedings of the 36th Annual Meeting of the Cognitive Science Society (CogSci14)},
  pages = {2181--2186},
  title = {{Error-Driven Adaptation of Higher-Level Expectations During Natural Reading}},
  year = {2014}
}

@article{Fine2013,
    Author = {Alex B. Fine and T. Florian Jaeger and Thomas Farmer and Ting Qian},
    Month = {October},
    Year = {2013},
    Volume = {8},
    Number = {10},
    Doi = {10.1371/journal.pone.0077661},
    Title = {Rapid Expectation Adaptation During Syntactic Comprehension},
    Journal = {PLoS ONE}
}

@article{Linzen2016,
  Author = {Tal Linzen and T. Florian Jaeger},
  Title = {Uncertainty and Expectation in Sentence Processing: Evidence From Subcategorization Distributions},
  Journal = {Cognitive Science},
  Volume = {40},
  Pages = { 1382–-1411},
  Year = {2016},
  doi = {10.1111/cogs.12274},
  abstract = {There is now considerable evidence that human sentence processing is expectation based: As people
              read a sentence, they use their statistical experience with their language to generate predictions
              about upcoming syntactic structure. This study examines how sentence processing is affected by
              readers’ uncertainty about those expectations. In a self-paced reading study, we use lexical subcategorization
              distributions to factorially manipulate both the strength of expectations and the uncertainty
              about them. We compare two types of uncertainty: uncertainty about the verb’s complement, reflecting
              the next prediction step; and uncertainty about the full sentence, reflecting an unbounded number
              of prediction steps. We find that uncertainty about the full structure, but not about the next step, was
              a significant predictor of processing difficulty: Greater reduction in uncertainty was correlated with
              increased reading times (RTs). We additionally replicated previously observed effects of expectation
              violation (surprisal), orthogonal to the effect of uncertainty. This suggests that both surprisal and
              uncertainty affect human RTs. We discuss the consequences for theories of sentence comprehension.},
Keywords = {Sentence processing; Uncertainty; Prediction; Entropy reduction; Surprisal;
Competition}
}

@inproceedings{Fine2010,
    Author = {Alex Fine and Ting Qian and T. Florian Jaeger and Robert Jacobs},
    Month = {July},
    Year = {2010},
    Title = {Is there syntactic adaptation in language comprehension?},
    Booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics:  Workshop on Cognitive Modeling and Computational Linguistics},
    Address = {Uppsala, Sweden},
    Url = {http://www.hlp.rochester.edu/publications/Fineetal10.pdf},
    Area = {comprehension}
}

@inproceedings{Kleinschmidt2012,
    Author = {Dave F Kleinschmidt and Alex B Fine and T. Florian Jaeger},
    Month = {August},
    Year = {2012},
    Title = {A belief-updating model of adaptation and cue combination in syntactic comprehension},
    Booktitle = {Proceedings of the 34th Annual Conference of the Cognitive Science Society},
    Editor = {N. Miyake and D. Peebles and R. P. Cooper},
    Keywords = {adaptation,bayesian modeling,cue combination,rational analysis,sentence processing},
    Publisher = {Cognitive Science Society},
    Address = {Sapporo, Japan},
    Howpublished = {talk},
    Url = {http://www.hlp.rochester.edu/publications/KleinschmidtFineJaeger_CogSci_2012.pdf}
}

@article {Qian2012,
    author = {Ting Qian  and  T. Florian Jaeger  and  Richard N Aslin},
    title = {Learning to Represent a Multi-Context Environment: More than Detecting Changes},
    journal = {Frontiers in Psychology},
    volume = {3},
    year = {2012},
    number = {00228},
    doi = {10.3389/fpsyg.2012.00228},
    issn = {1664-1078}
}

@article{Qian2016,
  author = {Ting Qian and T.Florian Jaeger and Richard N. Aslin},
  title = {{Incremental implicit learning of bundles of statistical patterns}},
  journal = {Cognition},
  volume = {157},
  year = {2016},
  pages = {156--173},
  doi = {10.1016/j.cognition.2016.09.002},
}